{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport copy\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Use smaller model variants for speed and efficiency\nfrom timm.models.convnext import convnext_base\nfrom timm.models.swin_transformer import swin_base_patch4_window7_224","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_dir = '/kaggle/input/ai-vs-human-generated-dataset'\ntrain_csv_path = os.path.join(base_dir, 'train.csv')\ntest_csv_path  = os.path.join(base_dir, 'test.csv')\n\ndf_train = pd.read_csv(train_csv_path)\ndf_test = pd.read_csv(test_csv_path)\n# Example row: file_name=\"train_data/041be3153810...\", label=0 or 1\n\ndf_train['file_name'] = df_train['file_name'].apply(lambda x: os.path.join(base_dir, x))\ndf_test['id'] = df_test['id'].apply(lambda x: os.path.join(base_dir, x))\n\n# Split training data into train and validation sets (10% for validation)\nall_image_paths = df_train['file_name'].values\nall_labels = df_train['label'].values\nX_train, X_val, y_train, y_val = train_test_split(all_image_paths, all_labels, test_size=0.1, random_state=42)\n\ntrain_split_df = pd.DataFrame({'file_name': X_train, 'label': y_train})\nval_split_df = pd.DataFrame({'file_name': X_val, 'label': y_val})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------- Custom Dataset ----------------------\nclass CustomDataset(Dataset):\n    def __init__(self, df, transform=None, is_test=False):\n        self.df = pd.DataFrame(df)\n        self.transform = transform\n        self.is_test = is_test\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        if self.is_test:\n            img_path = self.df.iloc[idx][\"id\"]\n        else:\n            img_path = self.df.iloc[idx][\"file_name\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        if self.is_test:\n            return img\n        else:\n            label = self.df.iloc[idx][\"label\"]\n            return img, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Training Transformations (with Augmentation)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((232)),                \n    transforms.RandomResizedCrop(224),  \n    transforms.RandomHorizontalFlip(),      \n    transforms.RandomRotation(10),           \n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  \n    transforms.ToTensor(),        \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  \n])                        \n    \n# ✅ Testing Transformations (NO Augmentation)\ntest_transforms = transforms.Compose([\n    transforms.Resize(232),  \n    transforms.CenterCrop(224),             \n    transforms.ToTensor(),                        \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])   \n])\nbatch_size = 4\n\ntrain_dataset = CustomDataset(df=train_split_df, transform=train_transforms, is_test=False)\nval_dataset = CustomDataset(df=val_split_df, transform=test_transforms, is_test=False)\ntest_dataset = CustomDataset(df=df_test, transform=test_transforms, is_test=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"CUDA is available, using GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CUDA is not available, using CPU.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom timm import create_model  # Ensure timm is installed\nfrom torch.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom tqdm import tqdm\nimport copy\nimport numpy as np\n\n# ---------------------- Utility: Compute Metrics ----------------------\ndef compute_metrics(preds, targets):\n    \"\"\"\n    Computes weighted F1 score using sklearn.\n    \"\"\"\n    preds_np = preds.cpu().numpy() if torch.is_tensor(preds) else preds\n    targets_np = targets.cpu().numpy() if torch.is_tensor(targets) else targets\n    return f1_score(targets_np, preds_np, average='weighted')\n\n# ---------------------- Feature Fusion Model (Simplified) ----------------------\nclass FeatureFusionModel(nn.Module):\n    def __init__(self, num_classes=1):\n        \"\"\"\n        Fuses features from ConvNeXt Large and Swin Transformer using a simple \n        global average pooling and fully connected fusion block. This architecture\n        mimics the simpler ViTConXWithAvgPooling model that generalizes well on unseen data.\n        \"\"\"\n        super().__init__()\n        # Load pre-trained backbones without their classification heads\n        self.convnext = create_model(\"convnext_large\", pretrained=True, num_classes=0)\n        convnext_out = self.convnext.num_features\n        \n        self.swin = create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)\n        swin_out = self.swin.num_features\n\n        # Freeze all parameters of both backbones\n        for param in self.convnext.parameters():\n            param.requires_grad = False\n        for param in self.swin.parameters():\n            param.requires_grad = False\n\n        # Unfreeze the last 20 parameters (i.e. fine-tune later layers)\n        for param in list(self.convnext.parameters())[-10:]:\n            param.requires_grad = True\n        for param in list(self.swin.parameters())[-10:]:\n            param.requires_grad = True\n\n        # Fully Connected Fusion Block (mirroring the simpler code)\n        self.feature_fusion = nn.Sequential(\n            nn.BatchNorm1d(convnext_out + swin_out),\n            nn.Linear(convnext_out + swin_out, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n        )\n        \n        # Decoder: Further processing before final output\n        self.decoder = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)  \n        )\n    \n    def forward(self, x):\n        # Pass input through both backbones\n        x_convnext = self.convnext(x)\n        x_swin = self.swin(x)\n        \n        # Apply global average pooling to convert features into fixed-size vectors\n        # (Assuming outputs are 4D tensors; if already 2D, this operation is skipped)\n        if x_convnext.dim() == 4:\n            x_convnext = F.adaptive_avg_pool2d(x_convnext, (1, 1)).view(x_convnext.size(0), -1)\n        if x_swin.dim() == 4:\n            x_swin = F.adaptive_avg_pool2d(x_swin, (1, 1)).view(x_swin.size(0), -1)\n        \n        # Concatenate the two feature vectors\n        x_combined = torch.cat((x_convnext, x_swin), dim=1)\n        x_fused = self.feature_fusion(x_combined)\n        \n        # Produce the final classification output\n        decoded_output = self.decoder(x_fused)\n        return decoded_output\n\n# ---------------------- Training Loop with Early Stopping ----------------------\ndef train_model(num_epochs=3, lr=1e-4, weight_decay=1e-2, patience=1):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Initialize the model and move it to device\n    model = FeatureFusionModel(num_classes=1).to(device)\n    \n    # Prepare optimizer using only parameters that require gradients\n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n    \n    # Scheduler: CosineAnnealingLR for smooth learning rate decay\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n    \n    # Loss function: BCEWithLogitsLoss (for binary classification)\n    criterion = nn.BCEWithLogitsLoss().to(device)\n    \n    # Initialize GradScaler for mixed precision training (no device argument)\n    scaler = GradScaler()\n    \n    # Lists to log progress (optional)\n    train_losses, train_accuracies = [], []\n    val_losses, val_accuracies, val_f1_scores, val_roc_aucs = [], [], [], []\n    \n    best_val_loss = float(\"inf\")\n    epochs_without_improvement = 0  \n    \n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss, epoch_accuracy = 0.0, 0.0\n        \n        # Training loop\n        for data, label in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n            data, label = data.to(device), label.to(device).float()\n            optimizer.zero_grad()\n            \n            # Mixed precision forward pass\n            with autocast(\"cuda\"):\n                output = model(data).squeeze(1)\n                loss = criterion(output, label)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            epoch_loss += loss.item()\n            preds = (torch.sigmoid(output) > 0.5).float()\n            epoch_accuracy += (preds == label).float().mean().item()\n        \n        epoch_loss /= len(train_loader)\n        epoch_accuracy /= len(train_loader)\n        train_losses.append(epoch_loss)\n        train_accuracies.append(epoch_accuracy)\n        \n        # Validation phase\n        model.eval()\n        val_loss, val_acc = 0.0, 0.0\n        val_pred_classes, val_labels_list = [], []\n        \n        with torch.no_grad():\n            for data, label in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n                data, label = data.to(device), label.to(device).float()\n                with autocast(\"cuda\"):\n                    output = model(data).squeeze(1)\n                    loss = criterion(output, label)\n                val_loss += loss.item()\n                preds = (torch.sigmoid(output) > 0.5).float()\n                val_acc += (preds == label).float().mean().item()\n                val_pred_classes.extend(preds.cpu().numpy())\n                val_labels_list.extend(label.cpu().numpy())\n        \n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n        val_f1 = f1_score(np.array(val_labels_list, dtype=int), \n                          np.array(val_pred_classes, dtype=int))\n        val_roc_auc = roc_auc_score(np.array(val_labels_list, dtype=int), \n                                    np.array(val_pred_classes, dtype=int))\n        \n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n        val_f1_scores.append(val_f1)\n        val_roc_aucs.append(val_roc_auc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_accuracy:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n              f\"Val F1: {val_f1:.4f} | Val ROC AUC: {val_roc_auc:.4f}\")\n        \n        scheduler.step()\n        \n        # Early stopping based on validation loss\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model_weights.pth')\n            torch.save(model, 'best_model.pth')\n            print(\"Model saved!\")\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n            if epochs_without_improvement >= patience:\n                print(f\"⚠️ Early stopping at epoch {epoch+1}\")\n                break\n    \n    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n    model.load_state_dict(torch.load('best_model_weights.pth'))\n    return model\n\n# ---------------------- Main Execution ----------------------\nif __name__ == \"__main__\":\n    # Ensure that train_loader, val_loader, train_dataset, and val_dataset are defined.\n    trained_model = train_model(num_epochs=1, lr=1e-4, weight_decay=1e-2, patience=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_inference(model):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.eval()\n    test_predictions = []\n    with torch.no_grad():\n        for data in tqdm(test_loader, desc=\"Inference Progress\", total=len(test_loader)):\n            data = data.to(device)\n            output = model(data).squeeze(1)\n            probs = torch.sigmoid(output)\n            preds = (probs > 0.5).int()\n            test_predictions.extend(preds.cpu().numpy())\n    test_predictions = np.array(test_predictions, dtype=int)\n    submission_df = df_test.copy()\n    submission_df[\"id\"] = submission_df[\"id\"].apply(lambda x: x.split(os.sep)[-1])\n    submission_df[\"id\"] = \"test_data_v2/\" + submission_df[\"id\"]\n    submission_df[\"label\"] = test_predictions\n    print(submission_df[\"label\"].value_counts())\n    submission_df.to_csv(\"submission.csv\", index=False)\n    print(\"Submission saved as submission.csv\")\n\nif __name__ == \"__main__\":\n    run_inference(trained_model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}